# qrmatrix.py.d3err:   ROUGE-1,0.24428,0.25572,0.24923
#    edited to get metrics on stopwords, has mysterious jump from R1.R=0.2226 to 0.2442.
#    only checked in on d3err branch (analysis only, not part of D3)

# This function taks a list of documents (from class) and writes a single file to summary
import local_util as u
logger = u.get_logger( __name__ ) #  https://docs.python.org/3/howto/logging.html

import sys # jgreve: want sys.stdout for writting stopword activity.
import os
import sum_config
import re # for removing multiple \s characters and source formatting
import math # for exp for weighting function
from operator import itemgetter

from nltk.tokenize import sent_tokenize, word_tokenize # for tokenizing sentences and words

from scipy import spatial

def get_tfidf(tally, ac, dc):
    return tally * (math.log(ac / (1 + dc)))

def qr_sum(docset, config):

    word_count = 0
    fname='qr_sum' # for labeling log statments.

    all_sentences = [] # contains lists with
    # SENTENCE LIST
    # 0 sentenc
    # 1 sentence vec
    # 2 sentece matrix vect
    # 3 position in article
    # 4 article index in article_length
    # 5 sentence score
    # 6 int for chronological ordering yyyymmddppp

    words_dict = {} # {word, index}
    words_vec = [] # [word]
    words_tally = {}
    words_docs = {} # key=word value=list of article names in which word appears

    article_length = [] # num_words

    # TODO: Define method of refusing fragment sentences
    short = 100 # temporary solution to fragment sentences making it into summary

    # TODO: make dictionary of stop words
    stops = open("src/stop_words")
    stop_lines = stops.read().split("\n")

    stop_words = {}

    STOP_TOKENIZE = False
    logger.debug('loading stop_words, STOP_TOKENIZE=%s', str(STOP_TOKENIZE) )
    stop_line_cnt = 0
    for line in stop_lines:
        stop_line_cnt += 1
        line = line.lower().strip()
        if STOP_TOKENIZE:
            words = word_tokenize( line )
            logger.debug('stop_words[{:03d}]: line="{}" --> words={}'.format(stop_line_cnt, line, words ))
            for word in words:
                stop_words[word] = 0
        else:
            # original D3 code.
            if line not in stop_words:
                stop_words[line] = 0

    article_count = 0
    logger.info('%s: docset=%s', fname, docset )
    for idx, article in enumerate(docset.articles):
        article_count += 1

        article_word_count = 0
        # TODO: GET ARTICLE ID INFORMATION AND STORE IT FOR LATER USE

        # ORDERING
        # -----------------------------------------
        # Extract date from article id
        title = article.id
        date = "x"
        # print(title)
        if len(title) < 17:
            date = title[3:11]
        else:
            date = title[8:16]
        # -----------------------------------------

        # print(date)

        # jgreve: who knew articles can be empty?
        if len(article.paragraphs) == 0:
            logger.warning('empty %s in docset#%s=%s)', article, idx, docset)
            continue

        sentence_position = 0
        for paragraph in article.paragraphs:

            # jgreve: should this logic logic go into the article_content.Article(),
            # or whatever populates Articels ?
            paragraph = re.sub("(\n|\t)", " ", paragraph)
            paragraph = re.sub("  +", " ", paragraph)
            paragraph = re.sub("^ ", "", paragraph)

            sentences = sent_tokenize(paragraph)

            for sentence in sentences:
                raw_words = word_tokenize(sentence)
                norm_words = []
                words = []

                for w in raw_words:
                    if re.search("[a-zA-Z]", w) != None:
                        if STOP_TOKENIZE:
                            # we'll check stopwords in the next loop
                            # (since the stop words are tokenized).
                            norm_words.append(w.lower())
                        else:
                            # let's check stopwords now.
                            w = w.lower()
                            if w not in stop_words:
                                norm_words.append(w) # keep it
                            else:
                                stop_words[w] += 1 # track how much our stopwords actually stop.
                    # words.append(w)

                for w in norm_words:
                    if w not in stop_words:
                        words.append(w)
                    else:
                        stop_words[w] += 1 # track how much our stopwords actually stop.

                article_word_count += len(words)

                # ORDERING
                # -----------------------------------------
                sentence_position += 1
                # print(sentence_position)
                position = str(sentence_position)
                # Make sentence position a 3-digit number
                if len(position) == 1:
                    position = "0" + position
                if len(position) == 2:
                    position = "0" + position
                # Combine date and position for super int yyyymmddppp
                priority_str = date + position
                priority = int(priority_str)
                # -----------------------------------------

                # Checks for first char cap and last char .
                first_char = False
                last_char = False

                if sentence[0].lower() != sentence[0]:
                    first_char = True

                if sentence[-1] == "." or sentence[-1] == "?" or sentence[-1] == "!":
                    last_char = True

                if len(words) > 6 and first_char == True and last_char == True: #arbitrary length of fragments
                    all_sentences.append([sentence, norm_words, [None], sentence_position, len(article_length), 0, priority])
                    if len(words) < short:
                        short = len(words)

                for word in norm_words:
                    if word not in words_dict:
                        words_dict[word] = len(words_vec)
                        words_tally[word] = 1
                        words_vec.append(word)

                        words_docs[word] = [article.id]
                    else:
                        words_tally[word] += 1

                        temp_article_id = words_docs[word]

                        if article.id not in temp_article_id:
                            temp_article_id.append(article.id)

                        words_docs[word] = temp_article_id


        article_length.append(article_word_count)

# CREATE FEATURE VECTORS
    for sentence in all_sentences:
        # print("\n\n", sentence[0], "\n", sentence[3], sentence[4])

        feat_vec = [0 for i in range(len(words_vec)) ]

        for word in sentence[1]:
            if word in words_dict:
                # tfidf = get_tfidf(words_tally[word], article_count, len(words_docs[word]))
                # feat_vec[words_dict[word]] = tfidf
                feat_vec[words_dict[word]] = words_tally[word]

        # print(sum(feat_vec))

        sentence[2] = feat_vec

# QR MATRIX
    selected_content = [] # list of sentences selected for

    # while 100 words not used up and shortest sentence isn't too long
    ranking_iteration = 0
    logger.debug( '*** new ranking_iteration *** docset=%s', docset )
    while word_count < 100 and word_count + short < 100:
        ranking_iteration += 1
        #logger.debug( '*** ranking_iteration=%d ***', ranking_iteration  )
        for sentence in all_sentences:

            # TODO: implement computation for t and g values from CLASSY [2001]
            # t and g currently based on hardwired values from CLASSY [2001] article
            t = 3
            g = 10

            score = 0
            if article_length[sentence[4]] == 0:
                print("ZERO SCORE:", sentence[0])
            else:
                nonzeros = sum(sentence[2])
                score = nonzeros * (g * math.exp(-8*(sentence[3]/article_length[sentence[4]]) + t))
                # print(score)
                sentence[5] = score
                # print(sentence[0], sentence[5])

        ranked = sorted(all_sentences, key=itemgetter(5), reverse=True)

        remove_words = []
        added = False
        while added == False:
            for rank_idx, x in enumerate( ranked ):
                if len(x[1]) + word_count < 100:
                    logger.debug('[%02d:%03d]<KEEP> ap.ai:%02d.%02d, sc:%12.4f words[%3d/%3d] <%s>',
                        ranking_iteration, rank_idx,
                        x[3], x[4], # api=3 position in article, ai=4 article index in article_length
                        x[5], # sc=5 sentence score
                        word_count, len(x[1]), # total/cur_sent
                        x[0]  # 0 sentence
                        #x[6], # ch=crhrono = 6 int for chronological ordering yyyymmddppp
                    )
                    selected_content.append(x)
                    word_count += len(x[1])
                    added = True
                    remove_words = x[1]
                    break
                else:
                    logger.debug('[%02d:%03d] skip  ap.ai:%02d.%02d, sc:%12.4f words[%3d/%3d] <%s>',
                        ranking_iteration, rank_idx,
                        x[3], x[4], # api=3 position in article, ai=4 article index in article_length
                        x[5], # sc=5 sentence score
                        word_count, len(x[1]), # total/cur_sent
                        x[0]  # 0 sentence
                        #ignore: x[6], # ch=crhrono = 6 int for chronological ordering yyyymmddppp
                    )
            if added == False:
                word_count = 101
                break

        # u.eprint(ranked[0][0], word_count)
        logger.debug('ranked[0][0]=%s, word_count=%d', ranked[0][0], word_count )

        r_index = []
        for item in remove_words:
            r_index.append(words_dict[item])
            # print(item, words_dict[item])

        for sentence in all_sentences:
            for ind in r_index:
                sentence[2][ind] = 0

    ordered_sum = sorted(selected_content, key= itemgetter(6)) # Sorts selected sentences by date/position

    summary = ""
    for s in ordered_sum:
        summary += s[0] + "\n"


# WRITE SUMMARY TO FILE
    #directory = config.DEFAULT_SUMMARY_DIR
    directory = config.OUTPUT_SUMMARY_DIRECTORY # jgreve: confg.yml files dont set the defaults. 

    # u.eprint('   docset.id      ="{}"'.format(docset.id)) # both appear to be the same
    # u.eprint('   docset.topic_id="{}"'.format(docset.topic_id))
    #docsetID = docset.topic_id # jgreve: apparently the id and the docset_id are different.
    # The D2 reqs say we need the docset_id.
    # (in case you're wondering I think the dash part is for -A and -B for test sets.)
    # (At any rate this is what we need to do to match the rouge logic.)

    # original logic: docsetID = docset.id # jgreve: turns out the 'id' is not actually the docset_id.
    # id_part = docsetID.split("-")
    #full_file_name = id_part[0] + "-A.M.100." + id_part[1] + ".9"
    if len( docset.topic_id ) != 6:
        logger.warning( 'expected 6 char topic_id instead of %d, docset=%s',  len(docset.topic_id), docset )
    part1 = docset.topic_id[0:-1] # up to but not including last char
    part2 = docset.topic_id[-1] # last char
    group_num = '9' # Because magic numbers are an anti-pattern.  jgreve
    # lets do some more sanity checks (because it would suck to try figuring out why this
    # doens't work two hours later at ROUGE eval  time (ask me how I know that)). jgreve
    if not re.search( r'[A-Z]\d\d\d\d$', part1 ):
        logger.warning('expected exactly a single letter A-Z and four digits for topic_id.part1 instead "%s" for docset=%',  part1, docset)
    if not re.search( r'^[A-Z]$', part2  ):
        logger.warning('expected exactly one uppercase letter A-Z for topic_id.part2 instead "%s" for docset=%s'.format( part2, docset))
    full_file_name = part1 + "-A.M.100." + part2 + "." + group_num

    if not os.path.exists(directory):
        os.makedirs(directory)
        logger.info('%s: making directory "%s"', fname, directory )
    filename = directory + "/" + full_file_name
    logger.info('%s: writing summary for %s to file="%s"', fname, docset, filename )
    with open(filename, "w+") as wout:
        wout.write(summary)

    sys.stdout.write('\n--- stop_words freq. for {}---'.format(docset))
    u.write_values( sys.stdout, "stop_words", stop_words )
    u.write_values( sys.stdout, "stop_words_rev", stop_words, descending_freq=True)

    return summary
