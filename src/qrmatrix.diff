*** qrmatrix.py.d3_orig	2018-05-16 16:31:27.945168027 -0700
--- qrmatrix.py.d3err	2018-05-16 16:48:11.959439326 -0700
***************
*** 1,11 ****
! # qrmatrix.py.d3_orig, ROUGE-1, 0.22264, 0.25731, 0.23795  
! # this is the official D3 version.
  # This function taks a list of documents (from class) and writes a single file to summary
  import local_util as u
  logger = u.get_logger( __name__ ) #  https://docs.python.org/3/howto/logging.html
  
  import os
  import sum_config
  import re # for removing multiple \s characters and source formatting
  import math # for exp for weighting function
  from operator import itemgetter
--- 1,14 ----
! # qrmatrix.py.d3err:   ROUGE-1,0.24428,0.25572,0.24923
! #    edited to get metrics on stopwords, has mysterious jump from R1.R=0.2226 to 0.2442.
! #    only checked in on d3err branch (analysis only, not part of D3)
! 
  # This function taks a list of documents (from class) and writes a single file to summary
  import local_util as u
  logger = u.get_logger( __name__ ) #  https://docs.python.org/3/howto/logging.html
  
+ import sys # jgreve: want sys.stdout for writting stopword activity.
  import os
  import sum_config
  import re # for removing multiple \s characters and source formatting
  import math # for exp for weighting function
  from operator import itemgetter
***************
*** 46,59 ****
      stops = open("src/stop_words")
      stop_lines = stops.read().split("\n")
  
      stop_words = {}
  
      for line in stop_lines:
!         line = line.lower()
!         if line not in stop_words:
!             stop_words[line] = 0
  
      article_count = 0
      logger.info('%s: docset=%s', fname, docset )
      for idx, article in enumerate(docset.articles):
          article_count += 1
--- 49,73 ----
      stops = open("src/stop_words")
      stop_lines = stops.read().split("\n")
  
      stop_words = {}
  
+     STOP_TOKENIZE = False
+     logger.debug('loading stop_words, STOP_TOKENIZE=%s', str(STOP_TOKENIZE) )
+     stop_line_cnt = 0
      for line in stop_lines:
!         stop_line_cnt += 1
!         line = line.lower().strip()
!         if STOP_TOKENIZE:
!             words = word_tokenize( line )
!             logger.debug('stop_words[{:03d}]: line="{}" --> words={}'.format(stop_line_cnt, line, words ))
!             for word in words:
!                 stop_words[word] = 0
!         else:
!             # original D3 code.
!             if line not in stop_words:
!                 stop_words[line] = 0
  
      article_count = 0
      logger.info('%s: docset=%s', fname, docset )
      for idx, article in enumerate(docset.articles):
          article_count += 1
***************
*** 96,111 ****
                  norm_words = []
                  words = []
  
                  for w in raw_words:
                      if re.search("[a-zA-Z]", w) != None:
!                         norm_words.append(w.lower())
                      # words.append(w)
  
                  for w in norm_words:
                      if w not in stop_words:
                          words.append(w)
  
                  article_word_count += len(words)
  
                  # ORDERING
                  # -----------------------------------------
--- 110,137 ----
                  norm_words = []
                  words = []
  
                  for w in raw_words:
                      if re.search("[a-zA-Z]", w) != None:
!                         if STOP_TOKENIZE:
!                             # we'll check stopwords in the next loop
!                             # (since the stop words are tokenized).
!                             norm_words.append(w.lower())
!                         else:
!                             # let's check stopwords now.
!                             w = w.lower()
!                             if w not in stop_words:
!                                 norm_words.append(w) # keep it
!                             else:
!                                 stop_words[w] += 1 # track how much our stopwords actually stop.
                      # words.append(w)
  
                  for w in norm_words:
                      if w not in stop_words:
                          words.append(w)
+                     else:
+                         stop_words[w] += 1 # track how much our stopwords actually stop.
  
                  article_word_count += len(words)
  
                  # ORDERING
                  # -----------------------------------------
***************
*** 175,185 ****
--- 201,215 ----
  
  # QR MATRIX
      selected_content = [] # list of sentences selected for
  
      # while 100 words not used up and shortest sentence isn't too long
+     ranking_iteration = 0
+     logger.debug( '*** new ranking_iteration *** docset=%s', docset )
      while word_count < 100 and word_count + short < 100:
+         ranking_iteration += 1
+         #logger.debug( '*** ranking_iteration=%d ***', ranking_iteration  )
          for sentence in all_sentences:
  
              # TODO: implement computation for t and g values from CLASSY [2001]
              # t and g currently based on hardwired values from CLASSY [2001] article
              t = 3
***************
*** 198,215 ****
          ranked = sorted(all_sentences, key=itemgetter(5), reverse=True)
  
          remove_words = []
          added = False
          while added == False:
!             for x in ranked:
                  if len(x[1]) + word_count < 100:
!                     logger.debug( 'len(x[1])=%d, word_cnt=%s, x[0]=%s, x[1]=%s', len(x[1]), word_count, x[0], x[1])
                      selected_content.append(x)
                      word_count += len(x[1])
                      added = True
                      remove_words = x[1]
                      break
              if added == False:
                  word_count = 101
                  break
  
          # u.eprint(ranked[0][0], word_count)
--- 228,261 ----
          ranked = sorted(all_sentences, key=itemgetter(5), reverse=True)
  
          remove_words = []
          added = False
          while added == False:
!             for rank_idx, x in enumerate( ranked ):
                  if len(x[1]) + word_count < 100:
!                     logger.debug('[%02d:%03d]<KEEP> ap.ai:%02d.%02d, sc:%12.4f words[%3d/%3d] <%s>',
!                         ranking_iteration, rank_idx,
!                         x[3], x[4], # api=3 position in article, ai=4 article index in article_length
!                         x[5], # sc=5 sentence score
!                         word_count, len(x[1]), # total/cur_sent
!                         x[0]  # 0 sentence
!                         #x[6], # ch=crhrono = 6 int for chronological ordering yyyymmddppp
!                     )
                      selected_content.append(x)
                      word_count += len(x[1])
                      added = True
                      remove_words = x[1]
                      break
+                 else:
+                     logger.debug('[%02d:%03d] skip  ap.ai:%02d.%02d, sc:%12.4f words[%3d/%3d] <%s>',
+                         ranking_iteration, rank_idx,
+                         x[3], x[4], # api=3 position in article, ai=4 article index in article_length
+                         x[5], # sc=5 sentence score
+                         word_count, len(x[1]), # total/cur_sent
+                         x[0]  # 0 sentence
+                         #ignore: x[6], # ch=crhrono = 6 int for chronological ordering yyyymmddppp
+                     )
              if added == False:
                  word_count = 101
                  break
  
          # u.eprint(ranked[0][0], word_count)
***************
*** 264,269 ****
--- 310,319 ----
      filename = directory + "/" + full_file_name
      logger.info('%s: writing summary for %s to file="%s"', fname, docset, filename )
      with open(filename, "w+") as wout:
          wout.write(summary)
  
+     sys.stdout.write('\n--- stop_words freq. for {}---'.format(docset))
+     u.write_values( sys.stdout, "stop_words", stop_words )
+     u.write_values( sys.stdout, "stop_words_rev", stop_words, descending_freq=True)
+ 
      return summary
